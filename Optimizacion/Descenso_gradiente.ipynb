{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6219a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gradient descent with F2 = x1 ^2 + x2 ^2\n",
      "Iteracion 0  tensor([6300, 6300])\n",
      "Iteracion  0   tensor([6174., 6174.]) f(x_t):  tensor(76236552.)\n",
      "Iteracion  1   tensor([6050.5200, 6050.5200]) f(x_t):  tensor(73217584.)\n",
      "Iteracion  2   tensor([5929.5098, 5929.5098]) f(x_t):  tensor(70318176.)\n",
      "Iteracion  3   tensor([5810.9194, 5810.9194]) f(x_t):  tensor(67533568.)\n",
      "Iteracion  4   tensor([5694.7012, 5694.7012]) f(x_t):  tensor(64859244.)\n",
      "Iteracion  5   tensor([5580.8071, 5580.8071]) f(x_t):  tensor(62290816.)\n",
      "Iteracion  6   tensor([5469.1909, 5469.1909]) f(x_t):  tensor(59824100.)\n",
      "Iteracion  7   tensor([5359.8071, 5359.8071]) f(x_t):  tensor(57455064.)\n",
      "Iteracion  8   tensor([5252.6108, 5252.6108]) f(x_t):  tensor(55179840.)\n",
      "Iteracion  9   tensor([5147.5586, 5147.5586]) f(x_t):  tensor(52994720.)\n",
      "Iteracion  10   tensor([5044.6074, 5044.6074]) f(x_t):  tensor(50896128.)\n",
      "Iteracion  11   tensor([4943.7153, 4943.7153]) f(x_t):  tensor(48880644.)\n",
      "Iteracion  12   tensor([4844.8408, 4844.8408]) f(x_t):  tensor(46944964.)\n",
      "Iteracion  13   tensor([4747.9438, 4747.9438]) f(x_t):  tensor(45085940.)\n",
      "Iteracion  14   tensor([4652.9849, 4652.9849]) f(x_t):  tensor(43300536.)\n",
      "Iteracion  15   tensor([4559.9253, 4559.9253]) f(x_t):  tensor(41585836.)\n",
      "Iteracion  16   tensor([4468.7266, 4468.7266]) f(x_t):  tensor(39939036.)\n",
      "Iteracion  17   tensor([4379.3521, 4379.3521]) f(x_t):  tensor(38357448.)\n",
      "Iteracion  18   tensor([4291.7651, 4291.7651]) f(x_t):  tensor(36838496.)\n",
      "Iteracion  19   tensor([4205.9297, 4205.9297]) f(x_t):  tensor(35379688.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_f(x):\n",
    "    z = x[0] ** 2 - 0.2 * x[1] ** 2\n",
    "    return z\n",
    "    \n",
    "def evaluate_grad_x(x_a):\n",
    "    grad_x = torch.tensor([2 *x_a[0], -0.4*x_a[1] ])\n",
    "    return grad_x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_f_2(x):\n",
    "    z = x[0] ** 2 +  x[1] ** 2\n",
    "    return z\n",
    "    \n",
    "def evaluate_grad_x_2(x_a):\n",
    "    grad_x = torch.tensor([2 *x_a[0], 2*x_a[1] ])\n",
    "    return grad_x\n",
    "\n",
    "def random_init_x():\n",
    "    x_a = torch.tensor([6300, 6300])\n",
    "    return x_a\n",
    "    \n",
    "def update_x(x_t, evaluate_grad_x_n,  alpha = 0.01):\n",
    "    #calculate the gradient in x_t\n",
    "    grad_x = evaluate_grad_x_n(x_t)\n",
    "    #update x, making x(t + 1)\n",
    "    x_t_plus_1 = x_t - alpha * grad_x\n",
    "    return x_t_plus_1\n",
    "\n",
    "def update_x_adagrad(x_t, s_t, evaluate_grad_x_n,  rho = 1, iteration_frame = 5):\n",
    "    #calculate the gradient in x_t\n",
    "    grad_x = evaluate_grad_x_n(x_t)\n",
    "    #update s(t)\n",
    "    s_t = s_t + (grad_x) ** 2\n",
    "    #print(\"s_t\")\n",
    "    #print(s_t)\n",
    "    #alpha_t inverse of s_t (element wise)\n",
    "    alpha_t = rho / s_t \n",
    "    #update x, making x(t + 1)\n",
    "    x_t_plus_1 = x_t - alpha_t * grad_x\n",
    "    return x_t_plus_1\n",
    "\n",
    "def run_gradient_descent(evaluate_f, evaluate_grad_x_n, T = 20):\n",
    "    x_0 = random_init_x()\n",
    "    #repeat T times\n",
    "    x_t = x_0\n",
    "    print(\"Iteracion 0 \", x_t)\n",
    "    for i in range(0, T):\n",
    "        x_t = update_x(x_t, evaluate_grad_x_n)\n",
    "        print(\"Iteracion \", i, \" \",  x_t, \"f(x_t): \", evaluate_f(x_t))\n",
    "    return x_t    \n",
    "\n",
    "\n",
    "def run_adaptive_gradient_descent(evaluate_grad_x_n, T = 20):\n",
    "    x_0 = random_init_x()\n",
    "    #repeat T times\n",
    "    x_t = x_0\n",
    "    epsilon = 0.00001\n",
    "    #init s_t\n",
    "    s_t = torch.ones(2) * epsilon\n",
    "    print(\"Iteracion 0 \", x_t)\n",
    "    for i in range(0, T):\n",
    "        x_t = update_x_adagrad(x_t, s_t, evaluate_grad_x_n)\n",
    "        print(\"Iteracion \", i, \" \",  x_t, \"f(x_t): \", evaluate_f(x_t))\n",
    "    return x_t    \n",
    "\n",
    "\n",
    "def run_newton_rhapson(evaluate_grad_x_n, T = 20):\n",
    "    x_0 = random_init_x()\n",
    "    #repeat T times\n",
    "    x_t = x_0\n",
    "    epsilon = 0.00001\n",
    "    #init s_t\n",
    "    s_t = torch.ones(2) * epsilon\n",
    "    print(\"Iteracion 0 \", x_t)\n",
    "    for i in range(0, T):\n",
    "        x_t = update_x_adagrad(x_t, s_t, evaluate_grad_x_n)\n",
    "        print(\"Iteracion \", i, \" \",  x_t, \"f(x_t): \", evaluate_f(x_t))\n",
    "    return x_t \n",
    "    \n",
    "def test_1():    \n",
    "    #unit test 1    \n",
    "    x_a = torch.tensor([10.0, 0])\n",
    "    x_b = update_x(x_a)\n",
    "    print(x_b)\n",
    "\n",
    "def test_2():\n",
    "    run_gradient_descent(evaluate_grad_x)\n",
    "    \n",
    "def test_3():\n",
    "    run_adaptive_gradient_descent(evaluate_grad_x)\n",
    "\n",
    "def test_4():    \n",
    "    run_gradient_descent(evaluate_f_2, evaluate_grad_x_2)\n",
    "\n",
    "print(\"Testing gradient descent with F2 = x1 ^2 + x2 ^2\")\n",
    "test_4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d755b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
