{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.1-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\saul1\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (4.3.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (2.11.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0->torchvision) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0->torchvision) (1.2.1)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qDVvweY0UIm0"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np;\n",
    "import pandas as pandas;\n",
    "from scipy import ndimage\n",
    "from torchvision import datasets, transforms\n",
    "from torch.distributions import normal\n",
    "from torch.distributions import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDHYTE1qUKJ6"
   },
   "source": [
    "## Creation of artificial data\n",
    "\n",
    "Data are created artificially with a mixed Gaussian distribution of two clusters, with specific means and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iZUnRQCtUi6l"
   },
   "outputs": [],
   "source": [
    "\n",
    "def createData(numberSamplesPerClass = 2, mean1 = [6, 6], mean2 = [14, 14], stds1 = [3, 3], stds2 = [2, 1]):\n",
    "    \"\"\"\n",
    "    Creates the data to be used for training, using a GMM distribution\n",
    "    @param numberSamplesPerClass, the number of samples per class\n",
    "    @param mean1, means for samples from the class 1\n",
    "    @param mean2, means for samples from the class 2\n",
    "    @param stds1, standard deviation for samples, class 1\n",
    "    @param stds2, standard deviation for samples, class 2\n",
    "    \"\"\"\n",
    "    means = torch.zeros(2, device = device)\n",
    "\n",
    "    # Ones to concatenate for bias\n",
    "    ones = torch.ones(numberSamplesPerClass, 1, device = device)\n",
    "    means[0] = mean1[0]\n",
    "    means[1] = mean1[1]\n",
    "    # Covariance matrix creation with identity\n",
    "    covarianceMatrix = torch.eye(2, device = device )\n",
    "    covarianceMatrix[0, 0] = stds1[0]\n",
    "    covarianceMatrix[1, 1] = stds1[1]\n",
    "    samplesClass1 = createDataOneClass(means, covarianceMatrix, numberSamplesPerClass)\n",
    "    means[0] = mean2[0]\n",
    "    means[1] = mean2[1]\n",
    "    covarianceMatrix[0, 0] = stds2[0]\n",
    "    covarianceMatrix[1, 1] = stds2[1]\n",
    "    samplesClass2 = createDataOneClass(means, covarianceMatrix, numberSamplesPerClass)\n",
    "    # Concatenates the ones for the bias\n",
    "    samplesClass1Bias = torch.cat((ones, samplesClass1), 1)\n",
    "    samplesClass2Bias = torch.cat((ones, samplesClass2), 1)\n",
    "    samplesAll = torch.cat((samplesClass1, samplesClass2), 0)\n",
    "    \n",
    "    plt.scatter(samplesClass1[:, 0].cpu().detach().numpy(), samplesClass1[:, 1].cpu().detach().numpy(), marker = \"x\")  \n",
    "    plt.scatter(samplesClass2[:, 0].cpu().detach().numpy(), samplesClass2[:, 1].cpu().detach().numpy())  \n",
    "    \n",
    "    #Create samples with bias\n",
    "    samplesAllBias = torch.cat((samplesClass1Bias, samplesClass2Bias), 0)\n",
    "    \n",
    "    #Create targets\n",
    "    targetsClass1 = torch.ones(numberSamplesPerClass, 1, device = device )\n",
    "    targetsClass2 = -1 * torch.ones(numberSamplesPerClass, 1, device = device )\n",
    "    targetsAll = torch.cat((targetsClass1, targetsClass2), 0)\n",
    "    \n",
    "    return (targetsAll, samplesAllBias)\n",
    "\n",
    "\n",
    "'''\n",
    "Creates data with gaussian distribution\n",
    "'''\n",
    "def createDataOneClass(means, covarianceMatrix, numberSamples):\n",
    "    # Inits the bi gaussian data generator\n",
    "    multiGaussGenerator = multivariate_normal.MultivariateNormal(means, covarianceMatrix)\n",
    "    # Takes the samples\n",
    "    samples = multiGaussGenerator.sample(torch.Size([numberSamples]))\n",
    "    samples = torch.tensor(samples,  device = device )\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9Su7DIj4ulk"
   },
   "source": [
    "## Least squares implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ogDwb74F4ull"
   },
   "outputs": [],
   "source": [
    "def estimate_optimum_LS(t, M):\n",
    "    \"\"\"\n",
    "    Estimate the optimum W with NO FORS\n",
    "    param M: NumSamples x 1 matrix with target values (1 or -1)\n",
    "    param t: NumSamples x NumDimensions \n",
    "    return w: array with optimum weights\n",
    "    \"\"\"\n",
    "    M_pinv = torch.tensor(np.linalg.pinv(M.numpy()), device = device )\n",
    "    w_opt = M_pinv.mm(t)\n",
    "    return w_opt         \n",
    "\n",
    "\n",
    "def forward(M, w):\n",
    "    \"\"\"\n",
    "    Get model output, with NO FORS\n",
    "    param M: dataset\n",
    "    return t_estimated, with 1 or -1\n",
    "    \"\"\"\n",
    "    y = M.mm(w)        \n",
    "    y[y > 0] = 1\n",
    "    y[y <= 0 ] = -1\n",
    "    t_estimated = y\n",
    "    return t_estimated\n",
    "\n",
    "def evaluate_error(t, t_estimated):\n",
    "    \"\"\"\n",
    "    Evaluate model error using the euclidian distance\n",
    "    param t, real targets\n",
    "    param t_estimated, estimated targets by the model\n",
    "    \"\"\"\n",
    "    error = torch.norm(t - t_estimated, 1) / t.shape[0]\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trqQMVu34ulm"
   },
   "source": [
    "## Test the original least squares model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "VHQyGpb-4ulm",
    "outputId": "6eab3037-5afe-4c6d-d5e5-5b3347a560fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-133e7cc0dff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test error: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-133e7cc0dff0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumberSamplesPerClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#estimate optimum model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mw_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_optimum_LS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pesos optimos \\n \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#estimate targets for the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b97f52df6a4e>\u001b[0m in \u001b[0;36mestimate_optimum_LS\u001b[0;34m(t, M)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0moptimum\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mM_pinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mw_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_pinv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_line(wOpt):\n",
    "  wOpt = wOpt.cpu().detach()\n",
    "  b = (-1*wOpt[0] / wOpt[2]).numpy()[0]\n",
    "  m = (-1*wOpt[1] / wOpt[2]).numpy()[0]\n",
    "  x = np.arange(0., 15., 0.01)\n",
    "  y = m * x + b\n",
    "  plt.plot(x, y)\n",
    "  plt.show()\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\"\"\"\n",
    "Main function\n",
    "\"\"\"\n",
    "def main():\n",
    "    (t, M) = createData(numberSamplesPerClass = 20)\n",
    "    #estimate optimum model weights\n",
    "    w_opt = estimate_optimum_LS(t, M)\n",
    "    print(\"Pesos optimos \\n \", w_opt)    \n",
    "    #estimate targets for the training dataset\n",
    "    t_estimated = forward(M, w_opt)\n",
    "    #evaluates error using L1 distance\n",
    "    error = evaluate_error(t, t_estimated)\n",
    "    plot_line(w_opt)\n",
    "    print(\"Training error: \", error)\n",
    "    #Test error\n",
    "    (t_test, M_test) = createData(numberSamplesPerClass = 20)\n",
    "    #estimate targets for the training dataset\n",
    "    t_estimated_test = forward(M_test, w_opt)\n",
    "    #evaluates error using L1 distance\n",
    "    error = evaluate_error(t_test, t_estimated_test)\n",
    "    print(\"Test error: \", error)\n",
    "    \n",
    "#main()\n",
    "(t, M) = createData(numberSamplesPerClass = 20, mean1 = [6, 6], mean2 = [14, 14], stds1 = [3, 3], stds2 = [2, 1])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
